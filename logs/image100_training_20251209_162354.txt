2025-12-09 16:23:54,818 - INFO - 日志保存到: G:\0_Python\Pytorch_learning\MobileNet\logs\image100_training_20251209_162354.txt
2025-12-09 16:23:56,410 - INFO - Using device: cuda
2025-12-09 16:23:56,410 - INFO - Loaded config: cifar100_mobilevit_xxs
2025-12-09 16:23:56,411 - INFO - 使用数据路径: G:\0_Python\Pytorch_learning\MobileNet\data\cifar-100-python
2025-12-09 16:23:58,309 - INFO - 模型结构:
MobileViT(
  (conv1): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): SiLU()
  )
  (mv2): ModuleList(
    (0): MV2Block(
      (conv): Sequential(
        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): MV2Block(
      (conv): Sequential(
        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2-3): 2 x MV2Block(
      (conv): Sequential(
        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): MV2Block(
      (conv): Sequential(
        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): MV2Block(
      (conv): Sequential(
        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): MV2Block(
      (conv): Sequential(
        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): SiLU()
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): SiLU()
        (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (m_vits): ModuleList(
    (0): MobileViTAttention(
      (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))
      (trans): Transformer(
        (layers): ModuleList(
          (0-1): 2 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=64, out_features=1536, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=64, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=64, out_features=128, bias=True)
                  (1): SiLU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=128, out_features=64, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (conv3): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))
      (conv4): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): MobileViTAttention(
      (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(48, 80, kernel_size=(1, 1), stride=(1, 1))
      (trans): Transformer(
        (layers): ModuleList(
          (0-3): 4 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=80, out_features=1536, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=80, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=80, out_features=320, bias=True)
                  (1): SiLU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=320, out_features=80, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (conv3): Conv2d(80, 48, kernel_size=(1, 1), stride=(1, 1))
      (conv4): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): MobileViTAttention(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
      (trans): Transformer(
        (layers): ModuleList(
          (0-2): 3 x ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=96, out_features=1536, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=512, out_features=96, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=96, out_features=384, bias=True)
                  (1): SiLU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=384, out_features=96, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (conv3): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      (conv4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (conv2): Sequential(
    (0): Conv2d(64, 320, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): SiLU()
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (fc): Linear(in_features=320, out_features=100, bias=False)
)
2025-12-09 16:23:58,319 - INFO - 超参数设置:
Batch Size: 128, Epochs: 50, Learning Rate: 0.1, Warmup Epochs: 5
2025-12-09 16:23:58,320 - INFO - 开始训练...
2025-12-09 16:23:58,320 - INFO - ================================================================================
2025-12-09 16:48:15,538 - INFO - Epoch [1/50] LR: 0.020080 | Train Loss: 4.6031, Train acc: 1.2520, Top-1: 1.25%, Top-5: 6.19% | 
Test Loss: 4.5910,Test acc: 1.4300, Top-1: 1.43%, Top-5: 6.74% | 
Epoch [1/50] , Test Once Delay: 42.6616s, Avarge Delay: 42.6616s | 
2025-12-09 16:48:15,614 - INFO - ✅ New best Top-1 accuracy: 1.43% — model saved!
2025-12-09 17:12:25,530 - INFO - Epoch [2/50] LR: 0.040060 | Train Loss: 4.0359, Train acc: 8.8620, Top-1: 8.86%, Top-5: 29.30% | 
Test Loss: 3.7066,Test acc: 14.3900, Top-1: 14.39%, Top-5: 41.42% | 
Epoch [2/50] , Test Once Delay: 41.9977s, Avarge Delay: 42.3296s | 
2025-12-09 17:12:25,568 - INFO - ✅ New best Top-1 accuracy: 14.39% — model saved!
2025-12-09 17:36:34,689 - INFO - Epoch [3/50] LR: 0.060040 | Train Loss: 3.6383, Train acc: 16.8520, Top-1: 16.85%, Top-5: 44.10% | 
Test Loss: 3.4780,Test acc: 19.7000, Top-1: 19.70%, Top-5: 49.43% | 
Epoch [3/50] , Test Once Delay: 41.8615s, Avarge Delay: 42.1736s | 
2025-12-09 17:36:34,714 - INFO - ✅ New best Top-1 accuracy: 19.70% — model saved!
2025-12-09 18:00:44,708 - INFO - Epoch [4/50] LR: 0.080020 | Train Loss: 3.3925, Train acc: 22.1200, Top-1: 22.12%, Top-5: 52.57% | 
Test Loss: 3.3585,Test acc: 23.1700, Top-1: 23.17%, Top-5: 54.66% | 
Epoch [4/50] , Test Once Delay: 42.0555s, Avarge Delay: 42.1441s | 
2025-12-09 18:00:44,732 - INFO - ✅ New best Top-1 accuracy: 23.17% — model saved!
2025-12-09 18:24:54,505 - INFO - Epoch [5/50] LR: 0.100000 | Train Loss: 3.2047, Train acc: 26.8100, Top-1: 26.81%, Top-5: 59.05% | 
Test Loss: 3.2436,Test acc: 26.3500, Top-1: 26.35%, Top-5: 58.22% | 
Epoch [5/50] , Test Once Delay: 42.0232s, Avarge Delay: 42.1199s | 
2025-12-09 18:24:54,527 - INFO - ✅ New best Top-1 accuracy: 26.35% — model saved!
2025-12-09 18:49:04,009 - INFO - Epoch [6/50] LR: 0.099878 | Train Loss: 3.0744, Train acc: 30.3680, Top-1: 30.37%, Top-5: 63.51% | 
Test Loss: 3.1141,Test acc: 29.9500, Top-1: 29.95%, Top-5: 61.24% | 
Epoch [6/50] , Test Once Delay: 42.0564s, Avarge Delay: 42.1093s | 
2025-12-09 18:49:04,038 - INFO - ✅ New best Top-1 accuracy: 29.95% — model saved!
